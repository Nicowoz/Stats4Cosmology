{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a259cf31",
   "metadata": {},
   "source": [
    "# Basic Statistics for Cosmology: Exercises on $H_0$\n",
    "\n",
    "This notebook is designed as a **student exercise**.  \n",
    "You will work through **Frequentist** and **Bayesian** methods to estimate the Hubble constant $H_0$ from mock data.  \n",
    "\n",
    "ðŸ‘‰ This is the **exercise version** (without solutions).  \n",
    "The instructor has a separate notebook with solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "- Fill in the code cells where you see `# TODO`.\n",
    "- Answer the discussion questions in markdown cells.\n",
    "- Try to first attempt on your own before checking solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## Doing SN cosmology with a synthetic dataset, using luminosity distance relation\n",
    "\n",
    "**Key low-$z$ relations** (valid for $z\\lesssim0.1$):\n",
    "\n",
    "$$ d_L \\approx \\frac{cz}{H_0} \\quad (\\text{in Mpc}) $$\n",
    "$$ \\mu \\equiv m - M = 5\\log_{10}\\!\\left( \\frac{d_L}{\\text{Mpc}} \\right) + 25 $$\n",
    "Combining these: \n",
    "$$ m = M + 5\\log_{10}(cz) - 5\\log_{10}(H_0) + 25. $$\n",
    "\n",
    "If $M$ is known (from external calibration), one can directly infer $H_0$. If $M$ is unknown **and** you only fit SN data, then $(M, H_0)$ are nearly perfectly degenerate at low $z$â€”you must supply an $M$ prior or external constraint to break the degeneracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd5d39",
   "metadata": {},
   "source": [
    "## Part 1: Frequentist Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc8312c",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Load and Visualize Data\n",
    "Generate a very simplistic SN dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "c_km_s = 299792.458  # speed of light in km/s\n",
    "\n",
    "def mu_from_z(z, H0):\n",
    "    \"\"\"Low-z luminosity distance modulus: mu = 5 log10(c z / H0 [Mpc]) + 25.\n",
    "    Here c is in km/s, H0 in km/s/Mpc, so cz/H0 is in Mpc.\n",
    "    \"\"\"\n",
    "    dL_Mpc = (c_km_s * z) / H0\n",
    "    return 5.0 * np.log10(dL_Mpc) + 25.0\n",
    "\n",
    "def mag_model(z, H0, M):\n",
    "    return M + mu_from_z(z, H0)\n",
    "\n",
    "def sigma_mu_pec(z, v_pec=300.0, H0=70.0):\n",
    "    \"\"\"Approximate magnitude uncertainty from peculiar velocities.\n",
    "    sigma_mu ~ (5/ln(10)) * (sigma_dL/dL).\n",
    "    At low z, dL ~ cz/H0, so fractional distance error from peculiar velocity sigma_v is ~ sigma_v/(cz).\n",
    "    \"\"\"\n",
    "    frac = v_pec/(c_km_s * z)\n",
    "    return (5/np.log(10)) * frac\n",
    "\n",
    "@dataclass\n",
    "class SNSample:\n",
    "    \"\"\"\n",
    "    Generate a synthetic supernova sample for cosmological parameter estimation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : np.ndarray\n",
    "        Array of redshifts of the supernovae.\n",
    "    m : np.ndarray\n",
    "        Array of observed magnitudes of the supernovae.\n",
    "    sigma_m : np.ndarray\n",
    "        Array of total magnitude uncertainties (measurement + intrinsic + peculiar velocity).\n",
    "    name : str\n",
    "        Str for the name of the sample (default is \"synthetic\").\n",
    "    \"\"\"\n",
    "    z: np.ndarray\n",
    "    m: np.ndarray\n",
    "    sigma_m: np.ndarray\n",
    "    name: str = \"synthetic\"\n",
    "\n",
    "def make_synthetic_sample(N=100, H0=73.0, M=-19.3, zmin=0.01, zmax=0.08,\n",
    "                          sigma_meas=0.1, sigma_int=0.1, v_pec=300.0, rng=None) -> SNSample:\n",
    "    \"\"\"\n",
    "    Generate a synthetic supernova sample for cosmological parameter estimation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        Number of supernovae to simulate.\n",
    "    H0 : float\n",
    "        True Hubble constant (km/s/Mpc) used to generate the data.\n",
    "    M : float\n",
    "        True absolute magnitude of the supernovae.\n",
    "    zmin : float\n",
    "        Minimum redshift of the sample.\n",
    "    zmax : float\n",
    "        Maximum redshift of the sample.\n",
    "    sigma_meas : float\n",
    "        Measurement uncertainty in magnitudes (per SN).\n",
    "    sigma_int : float\n",
    "        Intrinsic scatter in magnitudes (per SN).\n",
    "    v_pec : float\n",
    "        RMS peculiar velocity in km/s (affects low-z distance errors).\n",
    "    rng : int, np.random.Generator, or None\n",
    "        Random seed or numpy random generator for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    SNSample\n",
    "        Dataclass containing arrays of redshift (z), observed magnitude (m), \n",
    "        total magnitude uncertainty (sigma_m), and sample name.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rng)\n",
    "    z = np.sort(rng.uniform(zmin, zmax, size=N))\n",
    "    mu = mu_from_z(z, H0)\n",
    "    m_true = M + mu\n",
    "    # per-SN error budget: measurement + peculiar velocity + intrinsic scatter\n",
    "    sig_pec = sigma_mu_pec(z, v_pec=v_pec, H0=H0)\n",
    "    sigma_tot = np.sqrt(sigma_meas**2 + sig_pec**2 + sigma_int**2)\n",
    "    m_obs = rng.normal(m_true, sigma_tot)\n",
    "    return SNSample(z=z, m=m_obs, sigma_m=sigma_tot, name=\"synthetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fafe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generate and plot data with error bars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e003e18",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Define the Ï‡Â² function\n",
    "Write down the chi-square function for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a68eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define chi2 function\n",
    "def chi2(theta, x, y, sigma):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b680690",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Minimize Ï‡Â²\n",
    "Find the best-fit parameters by minimizing Ï‡Â²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d41112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use numpy or scipy to minimize chi2\n",
    "# best_fit of H0 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd08734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: say something about goodness of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77176d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot residuals, always!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9bad5",
   "metadata": {},
   "source": [
    "### Exercise 1.4: Bootstrap uncertainties\n",
    "Use bootstrap resampling to estimate parameter uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement bootstrap loop, any ideas how to do this?\n",
    "\n",
    "# The concept is to resample the data with replacement, fit each resampled dataset, and collect the best-fit parameters to estimate uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349c9fa",
   "metadata": {},
   "source": [
    "### Exercise 1.5: p-value\n",
    "Calculate a p-value from the sampled probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077eae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: let's calculate a p-value from the sampled probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036870e0",
   "metadata": {},
   "source": [
    "### Exercise 1.6: Compare with MLE\n",
    "Explain how minimizing Ï‡Â² relates to Maximum Likelihood Estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3b981",
   "metadata": {},
   "source": [
    "ðŸ‘‰ *Write your discussion here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9393b",
   "metadata": {},
   "source": [
    "## Part 2: Bayesian Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d908f40",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Bayes' theorem\n",
    "Write Bayes' theorem and identify prior, likelihood, posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c6fd8e",
   "metadata": {},
   "source": [
    "ðŸ‘‰ *Write Bayes' theorem in your own words here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8a031",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Compute Posterior on a Grid\n",
    "Define priors and likelihood, then compute posterior on a parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4264445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement grid posterior evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70782ff8",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Normalize Posterior and Extract Marginal Estimates\n",
    "Normalize posterior, compute marginal estimations of $H_0$ and $M$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc44d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: normalize posterior\n",
    "# TODO: compute marginal distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6fcc6",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Credible Interval\n",
    "Compute the 68% credible interval from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cde965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: credible interval calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15e9a8",
   "metadata": {},
   "source": [
    "### Exercise 2.5: Compare with Frequentist Result\n",
    "Compare Bayesian and Frequentist estimates of $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28600379",
   "metadata": {},
   "source": [
    "ðŸ‘‰ *Write your comparison here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b91436",
   "metadata": {},
   "source": [
    "## Part 3: Re-do with `Cobaya`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af3623",
   "metadata": {},
   "source": [
    "Estimate $H_0$ using Cobaya and Bayesian Statistics and compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloe-org",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
